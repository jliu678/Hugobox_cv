<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ScVelo | Jiyuan Liu</title>
    <link>http://localhost:1313/tags/scvelo/</link>
      <atom:link href="http://localhost:1313/tags/scvelo/index.xml" rel="self" type="application/rss+xml" />
    <description>ScVelo</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Wed, 28 May 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/media/icon_hu_fd2be5a69becaf9e.png</url>
      <title>ScVelo</title>
      <link>http://localhost:1313/tags/scvelo/</link>
    </image>
    
    <item>
      <title>üß¨ Dynamic RNA velocity model-- (1) math solutions</title>
      <link>http://localhost:1313/post/11d.velocity_dynamic_model_derivation/</link>
      <pubDate>Wed, 28 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/11d.velocity_dynamic_model_derivation/</guid>
      <description>&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;The steady‚Äëstate model‚Äôs reliance on true steady states is at odds with &lt;a href=&#34;https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010492&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;known biophysical behavior&lt;/a&gt;. The dynamic model removes this requirement to broaden RNA velocity‚Äôs applicability but inevitably introduces new assumptions that may not hold for every dataset. Effective use of the dynamic model therefore demands a clear understanding of its strengths and limitations. Our blog series toward this goal begins by delving into the mathematical foundations of the dynamic model.&lt;/p&gt;
&lt;h3 id=&#34;symbol-definitions&#34;&gt;Symbol definitions&lt;/h3&gt;
&lt;p&gt;Please refer to the &lt;a href=&#34;https://www.nature.com/articles/s41587-020-0591-3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nature Biotechnology paper&lt;/a&gt; for the definitions of the symbols used in this derivation.&lt;/p&gt;
&lt;h3 id=&#34;dynamic-solutions-for-rna-velocity-model&#34;&gt;Dynamic Solutions for RNA Velocity Model&lt;/h3&gt;
&lt;p&gt;To derive the solution for s(t) from the given system of differential equations, we will follow a two-step process:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Solve the first differential equation for u(t) using the initial condition u(t‚ÇÄ)=u‚ÇÄ.&lt;/li&gt;
&lt;li&gt;Substitute the expression for u(t) into the second differential equation and then solve for s(t) using the initial condition s(t‚ÇÄ)=s‚ÇÄ.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;step-1-solve-for-ut&#34;&gt;Step 1: Solve for u(t)&lt;/h4&gt;
&lt;p&gt;The first differential equation is:&lt;/p&gt;
  
$$
\frac{du(t)}{dt}=\alpha-\beta u(t)
$$  


&lt;p&gt;Rearrange it into the standard form for a first-order linear ODE:&lt;/p&gt;
  
$$
\frac{du(t)}{dt}+\beta u(t)=\alpha
$$  


&lt;p&gt;This is a linear first-order differential equation of the form $\frac{dy}{dx}+P(x)y=Q(x)$, where P(t)=Œ≤ and Q(t)=Œ±.
The integrating factor (IF) is $e^{\int P(t)dt}=e^{\int \beta dt}=e^{\beta t}$.&lt;/p&gt;
&lt;p&gt;Multiply the entire equation by the integrating factor:&lt;/p&gt;
  
$$
e^{\beta t}\frac{du(t)}{dt}+\beta u(t)e^{\beta t}=\alpha e^{\beta t}
$$  


&lt;p&gt;The left side is the derivative of the product $(u(t)e^{\beta t})$:&lt;/p&gt;
  
$$
\frac{d}{dt}(u(t)e^{\beta t})=\alpha e^{\beta t}
$$  


&lt;p&gt;Integrate both sides with respect to t:&lt;/p&gt;
  
$$
\int \frac{d}{dt}(u(t)e^{\beta t})dt=\int \alpha e^{\beta t}dt
$$
$$
u(t)e^{\beta t}=\frac{\alpha}{\beta}e^{\beta t}+C_1
$$  


&lt;p&gt;Now, solve for u(t):&lt;/p&gt;
  
$$
u(t)=\frac{\alpha}{\beta}+C_1e^{-\beta t}
$$  


&lt;p&gt;Use the initial condition u(t‚ÇÄ)=u‚ÇÄ to find C‚ÇÅ:&lt;/p&gt;
  
$$
u_0=\frac{\alpha}{\beta}+C_1e^{-\beta t_0}
$$
$$
C_1e^{-\beta t_0}=u_0-\frac{\alpha}{\beta}
$$
$$
C_1=(u_0-\frac{\alpha}{\beta})e^{\beta t_0}
$$  


&lt;p&gt;Substitute C‚ÇÅ back into the equation for u(t):&lt;/p&gt;
  
$$
u(t)=\frac{\alpha}{\beta}+(u_0-\frac{\alpha}{\beta})e^{\beta t_0}e^{-\beta t}
$$
$$
u(t)=\frac{\alpha}{\beta}+(u_0-\frac{\alpha}{\beta})e^{-\beta(t-t_0)}
$$  


&lt;h4 id=&#34;step-2-solve-for-st&#34;&gt;Step 2: Solve for s(t)&lt;/h4&gt;
&lt;p&gt;Now that we have u(t), we can solve the second differential equation:&lt;/p&gt;
  
$$
\frac{ds(t)}{dt}=\beta u(t)-\gamma s(t)
$$  


&lt;p&gt;Substitute the expression for u(t):&lt;/p&gt;
  
$$
\frac{ds(t)}{dt}=\beta[\frac{\alpha}{\beta}+(u_0-\frac{\alpha}{\beta})e^{-\beta(t-t_0)}]-\gamma s(t)
$$
$$
\frac{ds(t)}{dt}+\gamma s(t)=\alpha+(u_0\beta-\alpha)e^{-\beta(t-t_0)}
$$  


&lt;p&gt;This is again a linear first-order differential equation. The integrating factor is $e^{\gamma t}$.&lt;/p&gt;
&lt;p&gt;Multiply both sides by the integrating factor:&lt;/p&gt;
  
$$
\frac{d}{dt}(s(t)e^{\gamma t})=\alpha e^{\gamma t}+(u_0\beta-\alpha)e^{\gamma t}e^{-\beta(t-t_0)}
$$  


&lt;p&gt;Integrate both sides:&lt;/p&gt;
  
$$
s(t)e^{\gamma t}=\frac{\alpha}{\gamma}e^{\gamma t}+(u_0\beta-\alpha)\int e^{\gamma t}e^{-\beta(t-t_0)}dt+C_2
$$  


&lt;p&gt;Simplify the integral:&lt;/p&gt;
  
$$
s(t)e^{\gamma t}=\frac{\alpha}{\gamma}e^{\gamma t}+(u_0\beta-\alpha)e^{\beta t_0}\int e^{(\gamma-\beta)t}dt+C_2
$$
$$
s(t)e^{\gamma t}=\frac{\alpha}{\gamma}e^{\gamma t}+(u_0\beta-\alpha)e^{\beta t_0}\frac{e^{(\gamma-\beta)t}}{\gamma-\beta}+C_2
$$  


&lt;p&gt;Solve for s(t):&lt;/p&gt;
  
$$
s(t)=\frac{\alpha}{\gamma}+(u_0\beta-\alpha)e^{\beta t_0}\frac{e^{-\beta t}}{\gamma-\beta}+C_2e^{-\gamma t}
$$  


&lt;p&gt;Use the initial condition s(t‚ÇÄ)=s‚ÇÄ to find C‚ÇÇ:&lt;/p&gt;
  
$$
s_0=\frac{\alpha}{\gamma}+(u_0\beta-\alpha)\frac{e^{-\beta t_0+\beta t_0}}{\gamma-\beta}+C_2e^{-\gamma t_0}
$$
$$
C_2e^{-\gamma t_0}=s_0-\frac{\alpha}{\gamma}-(u_0\beta-\alpha)\frac{1}{\gamma-\beta}
$$
$$
C_2=[s_0-\frac{\alpha}{\gamma}-(u_0\beta-\alpha)\frac{1}{\gamma-\beta}]e^{\gamma t_0}
$$  


&lt;p&gt;The complete solution for s(t) is:&lt;/p&gt;
  
$$
s(t)=\frac{\alpha}{\gamma}+\frac{u_0\beta-\alpha}{\gamma-\beta}e^{-\beta(t-t_0)}+[s_0-\frac{\alpha}{\gamma}-\frac{u_0\beta-\alpha}{\gamma-\beta}]e^{-\gamma(t-t_0)}
$$  


&lt;h4 id=&#34;special-case-if-Œ≥Œ≤&#34;&gt;Special case: If Œ≥=Œ≤&lt;/h4&gt;
&lt;p&gt;If Œ≥=Œ≤, the integral becomes:&lt;/p&gt;
  
$$
\int(Œ≤u_0-Œ±)e^{(Œ≥-Œ≤)t+Œ≤t_0}dt=(Œ≤u_0-Œ±)e^{Œ≤t_0}\int dt=(Œ≤u_0-Œ±)e^{Œ≤t_0}t
$$  


&lt;p&gt;Then,&lt;/p&gt;
  
$$
s(t)e^{Œ≥t}=\frac{Œ±}{Œ≥}e^{Œ≥t}+(Œ≤u_0-Œ±)e^{Œ≤t_0}t+C_2
$$
$$
s(t)=\frac{Œ±}{Œ≥}+(Œ≤u_0-Œ±)te^{-Œ≥t+Œ≤t_0}+C_2e^{-Œ≥t}
$$  


&lt;p&gt;Since Œ≥=Œ≤:&lt;/p&gt;
  
$$
s(t)=\frac{Œ±}{Œ≥}+(Œ≥u_0-Œ±)te^{-Œ≥(t-t_0)}+C_2e^{-Œ≥t}
$$
$$
s(t)=\frac{Œ±}{Œ≥}+(Œ≥u_0-Œ±)(t_0+œÑ)e^{-Œ≥œÑ}+C_2e^{-Œ≥(t_0+œÑ)}
$$  


&lt;p&gt;This scenario is not covered by the target equation, which explicitly has a denominator (Œ≥‚àíŒ≤), implying Œ≥‚â†Œ≤.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>üß¨ Dynamic RNA velocity model-- (2) parameter inference</title>
      <link>http://localhost:1313/post/11e.velocity_dynamic_model_inference/scvelo_parameter_inference/</link>
      <pubDate>Wed, 28 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/11e.velocity_dynamic_model_inference/scvelo_parameter_inference/</guid>
      <description>&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;To effectively apply the dynamic model for revealing RNA velocity in single-cell RNA-seq data, this second installment of our blog series takes a deep dive into its parameter inference using a two-stage EM algorithm. In this approach, latent time is initially assigned using an explicit formula, and then refined through standard optimization during the &amp;ldquo;Expectation&amp;rdquo; step of the final EM iteration.&lt;/p&gt;
&lt;h3 id=&#34;symbol-definitions&#34;&gt;Symbol definitions&lt;/h3&gt;
&lt;p&gt;Please refer to the &lt;a href=&#34;https://www.nature.com/articles/s41587-020-0591-3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nature Biotechnology paper&lt;/a&gt; for the definitions of the symbols used in this derivation.&lt;/p&gt;
&lt;h1 id=&#34;a-derivation-transforming-the-solution-for-st&#34;&gt;A) Derivation: Transforming the Solution for s(t)&lt;/h1&gt;
&lt;p&gt;Let&amp;rsquo;s derive the relationship $\tilde{s}(t) - \tilde{s}_\infty = (\tilde{s}_0 - \tilde{s}_\infty)e^{-\gamma\tau}$ from the provided information.&lt;/p&gt;
&lt;h2 id=&#34;given-information&#34;&gt;Given Information&lt;/h2&gt;
&lt;h3 id=&#34;system-of-differential-equations&#34;&gt;System of Differential Equations:&lt;/h3&gt;

$$
\frac{du(t)}{dt} = \alpha^{(k)} - \beta u(t)
$$



$$
\frac{ds(t)}{dt} = \beta u(t) - \gamma s(t)
$$


&lt;h3 id=&#34;solutions-for-ut-and-st&#34;&gt;Solutions for u(t) and s(t):&lt;/h3&gt;

$$
u(t) = u_0 e^{-\beta\tau} + \frac{\alpha^{(k)}}{\beta} (1 - e^{-\beta\tau})
$$


&lt;p&gt;(This can be rewritten as $u(t) = \frac{\alpha^{(k)}}{\beta} + (u_0 - \frac{\alpha^{(k)}}{\beta})e^{-\beta\tau}$)&lt;/p&gt;

$$
s(t) = s_0 e^{-\gamma\tau} + \frac{\alpha^{(k)}}{\gamma} (1 - e^{-\gamma\tau}) + \left(\frac{\alpha^{(k)} - \beta u_0}{\gamma - \beta}\right) (e^{-\gamma\tau} - e^{-\beta\tau})
$$


&lt;p&gt;(This can be expanded to $s(t) = \frac{\alpha^{(k)}}{\gamma} + (s_0 - \frac{\alpha^{(k)}}{\gamma} + \frac{\alpha^{(k)} - \beta u_0}{\gamma - \beta})e^{-\gamma\tau} - (\frac{\alpha^{(k)} - \beta u_0}{\gamma - \beta})e^{-\beta\tau}$)&lt;/p&gt;
&lt;p&gt;Where $\tau = t - t_0$.&lt;/p&gt;
&lt;h3 id=&#34;steady-state-values&#34;&gt;Steady-State Values:&lt;/h3&gt;
&lt;p&gt;From setting the derivatives to zero:&lt;/p&gt;

$$
u_\infty^{(k)} = \frac{\alpha^{(k)}}{\beta}
$$



$$
s_\infty = \frac{\beta u_\infty^{(k)}}{\gamma} = \frac{\alpha^{(k)}}{\gamma}
$$


&lt;h3 id=&#34;new-variable-definitions&#34;&gt;New Variable Definitions:&lt;/h3&gt;

$$
\tilde{\beta} := \frac{\beta}{\gamma - \beta}
$$



$$
\tilde{s}(t) := s(t) - \tilde{\beta}u(t)
$$



$$
\tilde{s}_\infty := s_\infty - \tilde{\beta}u_\infty^{(k)}
$$


&lt;h2 id=&#34;derivation-steps&#34;&gt;Derivation Steps&lt;/h2&gt;
&lt;p&gt;Our goal is to demonstrate: $\tilde{s}(t) - \tilde{s}_\infty = (\tilde{s}_0 - \tilde{s}_\infty)e^{-\gamma\tau}$.&lt;/p&gt;
&lt;h3 id=&#34;1-express-ut-in-a-convenient-form&#34;&gt;1. Express u(t) in a convenient form:&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s rewrite u(t) using its steady-state value $u_\infty^{(k)}$:&lt;/p&gt;

$$
u(t) = u_\infty^{(k)} + (u_0 - u_\infty^{(k)})e^{-\beta\tau}
$$


&lt;h3 id=&#34;2-substitute-st-and-ut-into-the-definition-of-&#34;&gt;2. Substitute s(t) and u(t) into the definition of $\tilde{s}(t)$:&lt;/h3&gt;

$$
\tilde{s}(t) = \left[\frac{\alpha^{(k)}}{\gamma} + \left(s_0 - \frac{\alpha^{(k)}}{\gamma} + \frac{\alpha^{(k)} - \beta u_0}{\gamma - \beta}\right)e^{-\gamma\tau} - \left(\frac{\alpha^{(k)} - \beta u_0}{\gamma - \beta}\right)e^{-\beta\tau}\right] - \tilde{\beta}\left[u_\infty^{(k)} + (u_0 - u_\infty^{(k)})e^{-\beta\tau}\right]
$$


&lt;p&gt;Now, substitute the definition of $\tilde{\beta} = \frac{\beta}{\gamma - \beta}$ and $u_\infty^{(k)} = \frac{\alpha^{(k)}}{\beta}$:&lt;/p&gt;

$$
\tilde{s}(t) = \left[\frac{\alpha^{(k)}}{\gamma} + \left(s_0 - \frac{\alpha^{(k)}}{\gamma} + \frac{\alpha^{(k)} - \beta u_0}{\gamma - \beta}\right)e^{-\gamma\tau} - \left(\frac{\alpha^{(k)} - \beta u_0}{\gamma - \beta}\right)e^{-\beta\tau}\right] - \frac{\beta}{\gamma - \beta}\left[\frac{\alpha^{(k)}}{\beta} + \left(u_0 - \frac{\alpha^{(k)}}{\beta}\right)e^{-\beta\tau}\right]
$$


&lt;p&gt;Let&amp;rsquo;s simplify the terms multiplied by $\tilde{\beta}$:&lt;/p&gt;

$$
\tilde{\beta}u_\infty^{(k)} = \frac{\beta}{\gamma - \beta} \cdot \frac{\alpha^{(k)}}{\beta} = \frac{\alpha^{(k)}}{\gamma - \beta}
$$



$$
\tilde{\beta}(u_0 - u_\infty^{(k)}) = \frac{\beta}{\gamma - \beta}\left(u_0 - \frac{\alpha^{(k)}}{\beta}\right) = \frac{\beta}{\gamma - \beta}\left(\frac{\beta u_0 - \alpha^{(k)}}{\beta}\right) = \frac{\beta u_0 - \alpha^{(k)}}{\gamma - \beta}
$$


&lt;p&gt;Substitute these back into the expression for $\tilde{s}(t)$:&lt;/p&gt;

$$
\tilde{s}(t) = \left[\frac{\alpha^{(k)}}{\gamma} + \left(s_0 - \frac{\alpha^{(k)}}{\gamma} + \frac{\alpha^{(k)} - \beta u_0}{\gamma - \beta}\right)e^{-\gamma\tau} - \left(\frac{\alpha^{(k)} - \beta u_0}{\gamma - \beta}\right)e^{-\beta\tau}\right] - \left[\frac{\alpha^{(k)}}{\gamma - \beta} + \left(\frac{\beta u_0 - \alpha^{(k)}}{\gamma - \beta}\right)e^{-\beta\tau}\right]
$$


&lt;h3 id=&#34;3-cancel-the--terms&#34;&gt;3. Cancel the $e^{-\beta\tau}$ terms:&lt;/h3&gt;
&lt;p&gt;Observe the coefficients of $e^{-\beta\tau}$:&lt;/p&gt;

$$
-\left(\frac{\alpha^{(k)} - \beta u_0}{\gamma - \beta}\right) - \left(\frac{\beta u_0 - \alpha^{(k)}}{\gamma - \beta}\right)
$$


&lt;p&gt;Since $\frac{\beta u_0 - \alpha^{(k)}}{\gamma - \beta} = -\frac{\alpha^{(k)} - \beta u_0}{\gamma - \beta}$, this simplifies to:&lt;/p&gt;

$$
-\left(\frac{\alpha^{(k)} - \beta u_0}{\gamma - \beta}\right) - \left(-\frac{\alpha^{(k)} - \beta u_0}{\gamma - \beta}\right) = -\left(\frac{\alpha^{(k)} - \beta u_0}{\gamma - \beta}\right) + \left(\frac{\alpha^{(k)} - \beta u_0}{\gamma - \beta}\right) = 0
$$


&lt;p&gt;The terms containing $e^{-\beta\tau}$ indeed cancel out, as intended by the choice of $\tilde{\beta}$.&lt;/p&gt;
&lt;h3 id=&#34;4-simplify-&#34;&gt;4. Simplify $\tilde{s}(t)$:&lt;/h3&gt;

$$
\tilde{s}(t) = \left(\frac{\alpha^{(k)}}{\gamma} - \frac{\alpha^{(k)}}{\gamma - \beta}\right) + \left(s_0 - \frac{\alpha^{(k)}}{\gamma} + \frac{\alpha^{(k)} - \beta u_0}{\gamma - \beta}\right)e^{-\gamma\tau}
$$


&lt;p&gt;Let&amp;rsquo;s simplify the constant term:&lt;/p&gt;

$$
\frac{\alpha^{(k)}}{\gamma} - \frac{\alpha^{(k)}}{\gamma - \beta} = \alpha^{(k)}\left(\frac{1}{\gamma} - \frac{1}{\gamma - \beta}\right) = \alpha^{(k)}\left(\frac{(\gamma - \beta) - \gamma}{\gamma(\gamma - \beta)}\right) = \alpha^{(k)}\left(\frac{-\beta}{\gamma(\gamma - \beta)}\right) = -\frac{\alpha^{(k)} \beta}{\gamma(\gamma - \beta)}
$$


&lt;p&gt;So, we have:&lt;/p&gt;

$$
\tilde{s}(t) = -\frac{\alpha^{(k)} \beta}{\gamma(\gamma - \beta)} + \left(s_0 - \frac{\alpha^{(k)}}{\gamma} + \frac{\alpha^{(k)} - \beta u_0}{\gamma - \beta}\right)e^{-\gamma\tau}
$$


&lt;h3 id=&#34;5-calculate--and-&#34;&gt;5. Calculate $\tilde{s}_0$ and $\tilde{s}_\infty$:&lt;/h3&gt;
&lt;p&gt;Given  $u_\infty^{(k)} = \frac{\alpha^{(k)}}{\beta}$ 
,  $ s_\infty = \frac{\beta u_\infty^{(k)}}{\gamma} = \frac{\alpha^{(k)}}{\gamma}
$ 
 and the definition of $\tilde{s}_\infty$ and $\tilde{\beta}$, we derive below. You may also consider view $\tilde{s}_\infty$  as Steady-state of $\tilde{s}(t)$  ($\tau \to \infty$, $e^{-\gamma\tau} \to 0$ ).&lt;/p&gt;

$$
\tilde{s}_\infty = -\frac{\alpha^{(k)} \beta}{\gamma(\gamma - \beta)} (= \lim_{\tau \to \infty} \tilde{s}(t) )
$$


&lt;p&gt;&lt;strong&gt;$\tilde{s}_0$ (Initial value of $\tilde{s}(t)$ at $\tau = 0$):&lt;/strong&gt;&lt;/p&gt;

$$
\tilde{s}_0 = s(t_0) - \tilde{\beta}u(t_0) = s_0 - \tilde{\beta}u_0 = s_0 - \frac{\beta}{\gamma - \beta}u_0
$$


&lt;h3 id=&#34;6-verify-the-target-relationship&#34;&gt;6. Verify the target relationship: $\tilde{s}(t) - \tilde{s}_\infty = (\tilde{s}_0 - \tilde{s}_\infty)e^{-\gamma\tau}$&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Left-Hand Side (LHS):&lt;/strong&gt;&lt;/p&gt;

$$
\tilde{s}(t) - \tilde{s}_\infty = \left(-\frac{\alpha^{(k)} \beta}{\gamma(\gamma - \beta)} + \left(s_0 - \frac{\alpha^{(k)}}{\gamma} + \frac{\alpha^{(k)} - \beta u_0}{\gamma - \beta}\right)e^{-\gamma\tau}\right) - \left(-\frac{\alpha^{(k)} \beta}{\gamma(\gamma - \beta)}\right)
$$



$$
\tilde{s}(t) - \tilde{s}_\infty = \left(s_0 - \frac{\alpha^{(k)}}{\gamma} + \frac{\alpha^{(k)} - \beta u_0}{\gamma - \beta}\right)e^{-\gamma\tau}
$$


&lt;p&gt;&lt;strong&gt;Right-Hand Side (RHS):&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First, calculate $(\tilde{s}_0 - \tilde{s}_\infty)$:&lt;/p&gt;

$$
(\tilde{s}_0 - \tilde{s}_\infty) = \left(s_0 - \frac{\beta u_0}{\gamma - \beta}\right) - \left(-\frac{\alpha^{(k)} \beta}{\gamma(\gamma - \beta)}\right) = s_0 - \frac{\beta u_0}{\gamma - \beta} + \frac{\alpha^{(k)} \beta}{\gamma(\gamma - \beta)}
$$


&lt;p&gt;Now, multiply by $e^{-\gamma\tau}$:&lt;/p&gt;

$$
(\tilde{s}_0 - \tilde{s}_\infty)e^{-\gamma\tau} = \left(s_0 - \frac{\beta u_0}{\gamma - \beta} + \frac{\alpha^{(k)} \beta}{\gamma(\gamma - \beta)}\right)e^{-\gamma\tau}
$$


&lt;p&gt;Finally, let&amp;rsquo;s compare the coefficients of $e^{-\gamma\tau}$ from both the LHS and RHS. We need to confirm that:&lt;/p&gt;

$$
s_0 - \frac{\alpha^{(k)}}{\gamma} + \frac{\alpha^{(k)} - \beta u_0}{\gamma - \beta} \text{ equals } s_0 - \frac{\beta u_0}{\gamma - \beta} + \frac{\alpha^{(k)} \beta}{\gamma(\gamma - \beta)}
$$


&lt;p&gt;Let&amp;rsquo;s simplify the terms without $s_0$:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;From LHS coefficient:&lt;/strong&gt;&lt;/p&gt;

$$
-\frac{\alpha^{(k)}}{\gamma} + \frac{\alpha^{(k)} - \beta u_0}{\gamma - \beta} = -\frac{\alpha^{(k)}}{\gamma} + \frac{\alpha^{(k)}}{\gamma - \beta} - \frac{\beta u_0}{\gamma - \beta}
$$
$$
= \alpha^{(k)}\left(\frac{1}{\gamma - \beta} - \frac{1}{\gamma}\right) - \frac{\beta u_0}{\gamma - \beta}
$$
$$
= \alpha^{(k)}\left(\frac{\gamma - (\gamma - \beta)}{\gamma(\gamma - \beta)}\right) - \frac{\beta u_0}{\gamma - \beta}
$$
$$
= \alpha^{(k)}\left(\frac{\beta}{\gamma(\gamma - \beta)}\right) - \frac{\beta u_0}{\gamma - \beta} = \frac{\alpha^{(k)} \beta}{\gamma(\gamma - \beta)} - \frac{\beta u_0}{\gamma - \beta}
$$


&lt;p&gt;&lt;strong&gt;From RHS coefficient:&lt;/strong&gt;&lt;/p&gt;

$$
-\frac{\beta u_0}{\gamma - \beta} + \frac{\alpha^{(k)} \beta}{\gamma(\gamma - \beta)}
$$


&lt;p&gt;As you can see, the simplified coefficients from both sides are indeed identical.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;By introducing the carefully chosen auxiliary variable $\tilde{\beta} = \frac{\beta}{\gamma - \beta}$, the transformation $\tilde{s}(t) = s(t) - \tilde{\beta}u(t)$ strategically eliminates the $e^{-\beta\tau}$ dependence from the solution of s(t). This simplification reveals a clean, exponential decay relationship for $\tilde{s}(t)$ towards its new steady-state value $\tilde{s}_\infty$, as expressed by:&lt;/p&gt;

$$
\tilde{s}(t) - \tilde{s}_\infty = (\tilde{s}_0 - \tilde{s}_\infty)e^{-\gamma\tau}
$$


&lt;p&gt;This shows how a more complex system can be transformed into a simpler, recognizable form by defining appropriate auxiliary variables.&lt;/p&gt;
&lt;h1 id=&#34;b-derivation--in-equation-10-11-in-scvelo-paper&#34;&gt;B) Derivation: $\tau$ in equation (10, 11) in &lt;a href=&#34;https://www.nature.com/articles/s41587-020-0591-3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;scVelo paper&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;Above smoothly gives Inverse Relationship for $\tau$ in equation (10) in the &lt;a href=&#34;https://www.nature.com/articles/s41587-020-0591-3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;scVelo paper&lt;/a&gt;. Although equation (11) looks not obvious, it actually requires even simpler reasoning.&lt;/p&gt;
&lt;h2 id=&#34;derive-hahahugoshortcode34s32hbhb-in-equation-11&#34;&gt;Derive  $\tau$ 
 in equation (11)&lt;/h2&gt;

$$
\tau = -\frac{1}{\beta}\ln\left(\frac{u - u_\infty^{(k)}}{u_0 - u_\infty^{(k)}}\right)
$$


&lt;p&gt;This expression makes sense given the solution for  $u(t)$ 
 can be rewritten as below by substituting  $u_\infty^{(k)} = \frac{\alpha^{(k)}}{\beta}$ 
:&lt;/p&gt;

$$
u(t) = u_\infty^{(k)} + (u_0 - u_\infty^{(k)})e^{-\beta\tau}
$$


&lt;p&gt;If we solve this for  $\tau$ 
, we get:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Rearrange:  $u(t) - u_\infty^{(k)} = (u_0 - u_\infty^{(k)})e^{-\beta\tau}$ 
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Divide both sides by  $(u_0 - u_\infty^{(k)})$ 
:  $\frac{u(t) - u_\infty^{(k)}}{u_0 - u_\infty^{(k)}} = e^{-\beta\tau}$ 
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Take natural log:  $\ln\left(\frac{u(t) - u_\infty^{(k)}}{u_0 - u_\infty^{(k)}}\right) = -\beta\tau$ 
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Solve for  $\tau$ 
:  $\tau = -\frac{1}{\beta}\ln\left(\frac{u(t) - u_\infty^{(k)}}{u_0 - u_\infty^{(k)}}\right)$ 
&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;c-explicit-formula-is-less-accurate-than-exact-optimization-in-scvelo&#34;&gt;C) Explicit Formula is Less Accurate than Exact Optimization in scVelo&lt;/h1&gt;
&lt;p&gt;The difference in accuracy between the explicit formula and exact optimization comes down to &lt;strong&gt;model assumptions versus reality&lt;/strong&gt;. Let me explain:&lt;/p&gt;
&lt;h2 id=&#34;why-the-explicit-formula-is-less-accurate&#34;&gt;Why the Explicit Formula is Less Accurate&lt;/h2&gt;
&lt;h3 id=&#34;1-single-gene-assumption&#34;&gt;1. Single-Gene Assumption&lt;/h3&gt;
&lt;p&gt;The explicit formula:&lt;/p&gt;

$$\tau_i = -\frac{1}{\beta}\ln\left(\frac{u_i - u_\infty^{(k)}}{u_0 - u_\infty^{(k)}}\right)$$


&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Uses only unspliced RNA&lt;/strong&gt;  $(u)$ 
 from one gene&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ignores spliced RNA&lt;/strong&gt;  $(s)$ 
 information completely&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Assumes perfect adherence&lt;/strong&gt; to the theoretical kinetic model for that single gene&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-no-cross-gene-integration&#34;&gt;2. No Cross-Gene Integration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Each gene would give a &lt;strong&gt;different time estimate&lt;/strong&gt; for the same cell&lt;/li&gt;
&lt;li&gt;The formula doesn&amp;rsquo;t reconcile these conflicting estimates&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gene-specific noise&lt;/strong&gt; and measurement errors aren&amp;rsquo;t averaged out&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3-model-violations&#34;&gt;3. Model Violations&lt;/h3&gt;
&lt;p&gt;Real cells don&amp;rsquo;t perfectly follow the kinetic equations because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Transcriptional bursting&lt;/strong&gt; creates noise&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cell-to-cell variability&lt;/strong&gt; in kinetic parameters&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Measurement noise&lt;/strong&gt; in RNA counts&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model approximations&lt;/strong&gt; (e.g., constant degradation rates)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;why-exact-optimization-is-more-accurate&#34;&gt;Why Exact Optimization is More Accurate&lt;/h2&gt;
&lt;h3 id=&#34;1-multi-gene-integration&#34;&gt;1. Multi-Gene Integration&lt;/h3&gt;
&lt;p&gt;The optimization objective:&lt;/p&gt;

$$\tau_i^* = \arg\min_\tau \sum_j \left[ (u_{i,j} - u_j(\tau))^2 + (s_{i,j} - s_j(\tau))^2 \right]$$


&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Uses both unspliced AND spliced&lt;/strong&gt; RNA information&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integrates evidence across ALL genes&lt;/strong&gt; simultaneously&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Finds the time that best explains the entire transcriptome&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-noise-averaging&#34;&gt;2. Noise Averaging&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Gene-specific measurement errors &lt;strong&gt;cancel out&lt;/strong&gt; when averaged&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Outlier genes&lt;/strong&gt; have less impact on the final time estimate&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Statistical power increases&lt;/strong&gt; with more observations&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3-comprehensive-model-fitting&#34;&gt;3. Comprehensive Model Fitting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Accounts for &lt;strong&gt;both RNA species&lt;/strong&gt;  $u$ 
 and  $s$ 
 jointly&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Balances competing evidence&lt;/strong&gt; from different genes&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Robust to individual gene model violations&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;computational-trade-off&#34;&gt;Computational Trade-off&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Explicit Formula&lt;/strong&gt;:  $O(1)$ 
 per cell&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fast but uses limited information&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Exact Optimization&lt;/strong&gt;:  $O(n_{iter} \times n_{genes})$ 
 per cell&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Slower but uses all available information&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;mathematical-perspective&#34;&gt;Mathematical Perspective&lt;/h2&gt;
&lt;h3 id=&#34;information-content-comparison&#34;&gt;Information Content Comparison&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Explicit Formula Information&lt;/strong&gt;:

$$I_{explicit} = f(u_{i,j}) \text{ for single gene } j$$

&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Exact Optimization Information&lt;/strong&gt;:

$$I_{exact} = f(\{u_{i,j}, s_{i,j}\}_{j=1}^{N_{genes}})$$

&lt;/p&gt;
&lt;p&gt;Where  $I_{exact} &gt;&gt; I_{explicit}$ 
 in terms of information content.&lt;/p&gt;
&lt;h3 id=&#34;error-propagation&#34;&gt;Error Propagation&lt;/h3&gt;
&lt;p&gt;For the explicit formula, measurement error in a single gene directly affects the time estimate:&lt;/p&gt;

$$\sigma_{\tau}^2 \propto \frac{\sigma_u^2}{(u_i - u_\infty)^2}$$


&lt;p&gt;For exact optimization, errors are averaged across genes:&lt;/p&gt;

$$\sigma_{\tau}^2 \propto \frac{1}{N_{genes}} \sum_j \frac{\sigma_{u,j}^2 + \sigma_{s,j}^2}{(u_{i,j} - u_j(\tau))^2 + (s_{i,j} - s_j(\tau))^2}$$


&lt;h2 id=&#34;biological-analogy&#34;&gt;Biological Analogy&lt;/h2&gt;
&lt;p&gt;Think of it like &lt;strong&gt;estimating someone&amp;rsquo;s age&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Explicit Formula&lt;/strong&gt; = Looking at just their hair color&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Quick, but hair color alone is unreliable&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Exact Optimization&lt;/strong&gt; = Looking at hair, skin, posture, clothing style, speech patterns, etc.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Takes more time, but gives a much better estimate by integrating multiple sources of evidence&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;why-the-two-stage-strategy-works&#34;&gt;Why the Two-Stage Strategy Works&lt;/h2&gt;
&lt;p&gt;The two-stage approach in scVelo is brilliant because:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Stage 1 (Explicit)&lt;/strong&gt;: Gets &amp;ldquo;close enough&amp;rdquo; quickly when parameters are changing rapidly&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stage 2 (Exact)&lt;/strong&gt;: Refines to high accuracy when parameters have stabilized&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;convergence-analysis&#34;&gt;Convergence Analysis&lt;/h3&gt;
&lt;p&gt;Early iterations:  $|\theta^{(t)} - \theta^{(t-1)}|$ 
 is large&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Approximate times sufficient since parameters will change significantly anyway&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Later iterations:  $|\theta^{(t)} - \theta^{(t-1)}| \approx 0$ 
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Exact times needed for final parameter refinement&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;computational-efficiency&#34;&gt;Computational Efficiency&lt;/h3&gt;
&lt;p&gt;Total computational cost:&lt;/p&gt;

$$\text{Cost} = T_{approx} \times O(1) + T_{exact} \times O(n_{iter} \times n_{genes})$$


&lt;p&gt;Where  $T_{approx} &gt;&gt; T_{exact}$ 
, making the overall algorithm much faster than using exact optimization throughout.&lt;/p&gt;
&lt;h2 id=&#34;key-insight&#34;&gt;Key Insight&lt;/h2&gt;
&lt;p&gt;The explicit formula serves as an excellent &lt;strong&gt;initialization and approximation&lt;/strong&gt; tool, while exact optimization provides the &lt;strong&gt;precision needed for final convergence&lt;/strong&gt;. This hybrid approach captures the best of both worlds: computational efficiency and high accuracy.&lt;/p&gt;
&lt;h1 id=&#34;supplement-em-implementation&#34;&gt;Supplement. EM implementation&lt;/h1&gt;
&lt;h2 id=&#34;gmm-em&#34;&gt;GMM EM&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;initialize&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mu_k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sigma_k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pi_k&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;1.&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;K&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;while&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;converged&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# E-step&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x_i&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;1.&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;K&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;log_p_x_given_z&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x_i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mu_k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;^&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sigma_k&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;^&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;log_p_z&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;log&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;pi_k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;logliks&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;log_p_x_given_z&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;log_p_z&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;q_i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;softmax&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;logliks&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# M-step&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;k&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;1.&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;K&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;N_k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sum_i&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;q_i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;mu_k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sum_i&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;q_i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x_i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;N_k&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;sigma_k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sqrt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sum_i&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;q_i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x_i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mu_k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;^&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;N_k&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;pi_k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;N_k&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;scvelo-em&#34;&gt;scVelo EM&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;initialize&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;theta&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Œ±&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Œ≤&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Œ≥&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;while&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;converged&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# E-step&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;mu_traj&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;solve_ODE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;theta&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;time_grid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x_i&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mu_t&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;enumerate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mu_traj&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;logliks&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-||&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x_i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mu_t&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;||^&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;œÉ&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;q_i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;softmax&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;logliks&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# M-step&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;loss_fn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;theta&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x_i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;q_i&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;zip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;q_all&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t_j&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;enumerate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;time_grid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;n&#34;&gt;mu_tj&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;solve_ODE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;t_j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;theta&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;q_i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x_i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mu_tj&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;||^&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;theta&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;optimize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;loss_fn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;scvelo-em-with-learnable-time-prior-œÄ_j&#34;&gt;scVelo EM with Learnable Time Prior œÄ_j&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;initialize&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;theta&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Œ±&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Œ≤&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Œ≥&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;initialize&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;œÄ&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# uniform over time grid&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;while&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;converged&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# E-step&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;mu_traj&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;solve_ODE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;theta&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;time_grid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x_i&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mu_t&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;enumerate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mu_traj&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;logliks&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-||&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x_i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mu_t&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;||^&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;œÉ&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;log&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;œÄ&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;q_i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;softmax&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;logliks&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# M-step&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;œÄ&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sum_i&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;q_i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;N&lt;/span&gt;  &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;all&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# update time prior&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;loss_fn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;theta&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x_i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;q_i&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;zip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;q_all&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t_j&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;enumerate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;time_grid&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;n&#34;&gt;mu_tj&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;solve_ODE&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;t_j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;theta&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;q_i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;j&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;||&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x_i&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mu_tj&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;||^&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;loss&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;theta&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;optimize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;loss_fn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>üß¨ Math derivation for steady-state RNA velocity model</title>
      <link>http://localhost:1313/post/11c.velocity_steady_state/</link>
      <pubDate>Wed, 28 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/11c.velocity_steady_state/</guid>
      <description>&lt;p&gt;The steady‚Äëstate model was the first to enable a mathematical estimation of RNA velocity, and most subsequent methods are modified versions of it or its generalization (the dynamic model in‚ÄØ&lt;code&gt;scVelo&lt;/code&gt;; see our other blogs). It has its limitations and a solid understanding of its underlying mathematics is needed to apply the model effectively. Here, we derive the steady-state model in &lt;code&gt;scVelo&lt;/code&gt; and &lt;code&gt;velocyto&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;a-estimating-rna-velocity-using-steady-state-ratio-in-scvelo-and-velocyto&#34;&gt;A. Estimating RNA velocity Using Steady-State Ratio in &lt;code&gt;scVelo&lt;/code&gt; and &lt;code&gt;velocyto&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;For steady-state models in &lt;code&gt;scVelo&lt;/code&gt; and &lt;code&gt;velocyto&lt;/code&gt;, RNA velocities are computed as deviations from this steady-state ratio:&lt;/p&gt;
 $$ \nu_i = u_i - \gamma_0 s_i  $$ 

&lt;p&gt;where
  
$$
\gamma_0 = \frac{\beta}{\gamma}
$$  

&lt;br&gt;
(i.e. the expected ratio of unspliced to spliced mRNA) is estimated analytically using least squares regression. Lower and upper quantiles in phase space, that is, where mRNA levels
reach minimum and maximum expression, respectively were assumed to be steady states and used for the estimation. Hence, the ratio can be approximated by a linear regression on these extreme quantiles.&lt;/p&gt;
&lt;p&gt;Specifically, $ \gamma_0 $
 is estimated as :&lt;/p&gt;
  
$$
\gamma_0 = \frac{\mathbf{u}^\top \mathbf{s}}{\lVert \mathbf{s} \rVert^2}
$$  


&lt;p&gt;This is a least-squares fit of the form:&lt;/p&gt;
  
$$
\mathbf{u} \approx \gamma_0 \cdot \mathbf{s}
$$  


&lt;p&gt;This means they model unspliced mRNA ($\mathbf{u}$
) as being linearly dependent on spliced mRNA ($\mathbf{s}$
), and solve for the slope $\gamma_0$
.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;symbol-definition&#34;&gt;Symbol Definition&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;$\mathbf{u} = (u_1, \ldots, u_n)$
: a vector of size-normalized unspliced mRNA counts for a single gene across a subset of cells.&lt;/li&gt;
&lt;li&gt;$\mathbf{s} = (s_1, \ldots, s_n)$
: a vector of corresponding spliced mRNA counts for that gene in the same subset of cells.&lt;/li&gt;
&lt;li&gt;$\mathbf{u}^\top \mathbf{s}$
: the dot product between unspliced and spliced vectors.&lt;/li&gt;
&lt;li&gt;$\lVert \mathbf{s} \rVert^2 = \mathbf{s}^\top \mathbf{s}$
: the squared norm of the spliced vector.&lt;/li&gt;
&lt;li&gt;$\gamma_0$
: the slope of the best-fit line through the origin relating $\mathbf{u}$
 to $\mathbf{s}$
, i.e., the steady-state ratio.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;b-derive-least-squares-solution-for-estimating-the-steady-state-ratio&#34;&gt;B. Derive Least Squares Solution for Estimating the Steady-State Ratio $\gamma_0$&lt;/h3&gt;
&lt;p&gt;Let‚Äôs walk through the full derivation of the least squares solution for estimating the steady-state ratio $\gamma_0$
, which minimizes the squared difference between the unspliced and spliced counts scaled by $\gamma_0$
.&lt;/p&gt;
&lt;hr&gt;
&lt;h4 id=&#34;-problem-setup&#34;&gt;üîß Problem Setup&lt;/h4&gt;
&lt;p&gt;Given:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$u \in \mathbb{R}^n$
: vector of unspliced counts across $n$
 cells&lt;/li&gt;
&lt;li&gt;$s \in \mathbb{R}^n$
: vector of spliced counts across the same $n$
 cells&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We model the unspliced counts as linearly proportional to spliced counts:&lt;/p&gt;

$$
u \approx \gamma_0 s
$$


&lt;p&gt;We want to find $\gamma_0$
 that minimizes the squared error:&lt;/p&gt;

$$
\min_{\gamma_0} \| u - \gamma_0 s \|^2
$$


&lt;hr&gt;
&lt;h4 id=&#34;-expand-the-norm&#34;&gt;üßÆ Expand the Norm&lt;/h4&gt;

$$
\| u - \gamma_0 s \|^2 = (u - \gamma_0 s)^\top (u - \gamma_0 s)
$$


&lt;p&gt;Expanding this expression:&lt;/p&gt;

$$
= u^\top u - 2\gamma_0 u^\top s + \gamma_0^2 s^\top s
$$


&lt;hr&gt;
&lt;h4 id=&#34;-minimize-with-respect-to&#34;&gt;üìâ Minimize with Respect to $\gamma_0$&lt;/h4&gt;
&lt;p&gt;Take derivative with respect to $\gamma_0$
 and set it to zero:&lt;/p&gt;

$$
\frac{d}{d\gamma_0} \left( \| u - \gamma_0 s \|^2 \right) = -2 u^\top s + 2\gamma_0 s^\top s = 0
$$


&lt;p&gt;Solve for $\gamma_0$
:&lt;/p&gt;

$$
\gamma_0 = \frac{u^\top s}{\|s\|^2}
$$


&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$u^\top s$
 is the dot product&lt;/li&gt;
&lt;li&gt;$\|s\|^2 = s^\top s$
 is the squared norm&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h4 id=&#34;-interpretation&#34;&gt;‚úÖ Interpretation&lt;/h4&gt;
&lt;p&gt;This is the slope of the best-fit line (through the origin) predicting unspliced from spliced counts. It&amp;rsquo;s equivalent to projecting $u$
 onto $s$
 in vector space.&lt;/p&gt;
&lt;p&gt;In RNA velocity, this slope $\gamma_0$
 serves as a reference ratio under the steady-state assumption:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Expected:&lt;/strong&gt;&lt;/p&gt;

$$
u = \gamma_0 s
$$


&lt;p&gt;Any deviation from this in other cells suggests that the gene is being upregulated or downregulated.&lt;/p&gt;
&lt;h3 id=&#34;c-extending-the-steady-state-model-with-offset&#34;&gt;C. Extending the Steady-State Model with Offset&lt;/h3&gt;
&lt;h4 id=&#34;-original-model-no-offset&#34;&gt;üß© Original Model (No Offset)&lt;/h4&gt;
&lt;p&gt;Previously, we assumed:&lt;/p&gt;

$$u_i \approx \gamma_0 \cdot s_i$$


&lt;p&gt;But this forces the regression line to go through the origin, which isn&amp;rsquo;t always biologically realistic.&lt;/p&gt;
&lt;h4 id=&#34;-generalized-model-with-offset&#34;&gt;‚úÖ Generalized Model (With Offset)&lt;/h4&gt;
&lt;p&gt;Now we model:&lt;/p&gt;

$$u_i \approx \gamma_0 \cdot s_i + o$$


&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\gamma_0$
 is still the slope (steady-state ratio)&lt;/li&gt;
&lt;li&gt;$o$
 is a constant offset (intercept), modeling basal transcription&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;-least-squares-solution-with-offset&#34;&gt;üîç Least Squares Solution with Offset&lt;/h4&gt;
&lt;p&gt;To solve for both $\gamma_0$
 and $o$
, we use ordinary least squares (OLS) for linear regression.&lt;/p&gt;
&lt;p&gt;Given data $u=(u_1,\ldots,u_n)$
, $s=(s_1,\ldots,s_n)$
, we estimate:&lt;/p&gt;

$$\hat{u} = \gamma_0 s + o$$


&lt;p&gt;OLS gives:&lt;/p&gt;
&lt;h5 id=&#34;slope-steady-state-ratio&#34;&gt;Slope (Steady-State Ratio):&lt;/h5&gt;

$$\gamma_0 = \frac{\text{Cov}(u,s)}{\text{Var}(s)}$$


&lt;p&gt;Where:&lt;/p&gt;

$$\text{Cov}(u,s) = \frac{1}{n}\sum_{i=1}^n (u_i-\bar{u})(s_i-\bar{s})$$



$$\text{Var}(s) = \frac{1}{n}\sum_{i=1}^n (s_i-\bar{s})^2$$


&lt;h5 id=&#34;offset&#34;&gt;Offset:&lt;/h5&gt;

$$o = \bar{u} - \gamma_0\bar{s}$$


&lt;p&gt;This centers the regression line at the mean of the data points.&lt;/p&gt;
&lt;h3 id=&#34;d-derive-least-squares-solution-with-intercept&#34;&gt;D. Derive Least Squares Solution with Intercept&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s derive the least squares solution with an intercept (offset) step by step. Our goal is to estimate both:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\gamma_0$
: the slope (steady-state ratio), and&lt;/li&gt;
&lt;li&gt;$o$
: the offset (basal transcription level)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;given that we model:&lt;/p&gt;

$$u_i = \gamma_0 s_i + o + \varepsilon_i$$


&lt;p&gt;for each cell $i$
, where $\varepsilon_i$
 is the residual.&lt;/p&gt;
&lt;h4 id=&#34;-1-problem-setup&#34;&gt;üß© 1. Problem Setup&lt;/h4&gt;
&lt;p&gt;Let $u = [u_1, u_2, ..., u_n]^T$
, and $s = [s_1, s_2, ..., s_n]^T$
&lt;/p&gt;
&lt;p&gt;We want to solve for $\gamma_0$
 and $o$
 that minimize the squared residuals:&lt;/p&gt;

$$\min_{\gamma_0,o} \sum_{i=1}^n (u_i - \gamma_0 s_i - o)^2$$


&lt;p&gt;This is a linear least squares regression with intercept.&lt;/p&gt;
&lt;h4 id=&#34;-2-matrix-form&#34;&gt;üßÆ 2. Matrix Form&lt;/h4&gt;
&lt;p&gt;Rewriting the model:&lt;/p&gt;

$$u = \gamma_0 s + o \cdot 1 + \varepsilon$$


&lt;p&gt;Let&amp;rsquo;s construct a design matrix $X$
:&lt;/p&gt;

$$X = \begin{bmatrix} s_1 &amp; 1 \\ s_2 &amp; 1 \\ \vdots &amp; \vdots \\ s_n &amp; 1 \end{bmatrix} \in \mathbb{R}^{n \times 2}$$


&lt;p&gt;Then the model becomes:&lt;/p&gt;

$$u = X \cdot \begin{bmatrix} \gamma_0 \\ o \end{bmatrix} + \varepsilon$$


&lt;p&gt;The least squares solution is given by:&lt;/p&gt;

$$\begin{bmatrix} \gamma_0 \\ o \end{bmatrix} = (X^T X)^{-1} X^T u$$


&lt;p&gt;We can compute this explicitly to get formulas for $\gamma_0$
 and $o$
 as below.&lt;/p&gt;
&lt;h5 id=&#34;slope&#34;&gt;Slope:&lt;/h5&gt;

$$\gamma_0 = \frac{\text{Cov}(u,s)}{\text{Var}(s)} = \frac{\sum_{i=1}^n (u_i-\bar{u})(s_i-\bar{s})}{\sum_{i=1}^n (s_i-\bar{s})^2}$$


&lt;h5 id=&#34;offset-1&#34;&gt;Offset:&lt;/h5&gt;

$$o = \bar{u} - \gamma_0\bar{s}$$


&lt;p&gt;These are the well-known results from simple linear regression that center the regression line at the mean of the data points.&lt;/p&gt;
&lt;h4 id=&#34;-3-matrix-derivation-of-the-least-squares-solution&#34;&gt;üéØ 3. Matrix Derivation of the Least Squares Solution&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s explicitly derive the solution:&lt;/p&gt;

$$\begin{bmatrix} \gamma_0 \\ o \end{bmatrix} = (X^\top X)^{-1} X^\top u$$


&lt;p&gt;for the model:&lt;/p&gt;

$$u = \gamma_0 s + o \cdot 1 + \varepsilon$$


&lt;h5 id=&#34;31-define-the-design-matrix-and-vectors&#34;&gt;3.1 Define the Design Matrix and Vectors&lt;/h5&gt;
&lt;p&gt;Let&amp;rsquo;s say we have $n$
 samples (cells), with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$s \in \mathbb{R}^n$
: spliced counts&lt;/li&gt;
&lt;li&gt;$u \in \mathbb{R}^n$
: unspliced counts&lt;/li&gt;
&lt;li&gt;$1 \in \mathbb{R}^n$
: vector of ones&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We define the design matrix $X \in \mathbb{R}^{n \times 2}$
:&lt;/p&gt;

$$X = \begin{bmatrix} s_1 &amp; 1 \\ s_2 &amp; 1 \\ \vdots &amp; \vdots \\ s_n &amp; 1 \end{bmatrix} = [s \quad 1]$$


&lt;p&gt;We write the model:&lt;/p&gt;

$$u = X\theta + \varepsilon, \text{ with } \theta = \begin{bmatrix} \gamma_0 \\ o \end{bmatrix}$$


&lt;p&gt;We want to minimize the residual sum of squares:&lt;/p&gt;

$$\min_\theta \|u - X\theta\|^2$$


&lt;h5 id=&#34;32-normal-equations&#34;&gt;3.2 Normal Equations&lt;/h5&gt;
&lt;p&gt;We derive the optimal $\theta$
 by solving the normal equations:&lt;/p&gt;

$$X^\top X\theta = X^\top u$$


&lt;p&gt;So,&lt;/p&gt;

$$\theta = (X^\top X)^{-1} X^\top u$$


&lt;p&gt;Let&amp;rsquo;s compute each term step by step.&lt;/p&gt;
&lt;h5 id=&#34;33-compute&#34;&gt;3.3 Compute $X^\top X$&lt;/h5&gt;

$$X^\top X = \begin{bmatrix} \sum s_i^2 &amp; \sum s_i \\ \sum s_i &amp; n \end{bmatrix}$$


&lt;p&gt;Let&amp;rsquo;s denote:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$S = \sum_{i=1}^n s_i$
&lt;/li&gt;
&lt;li&gt;$S_2 = \sum_{i=1}^n s_i^2$
&lt;/li&gt;
&lt;li&gt;$n$
: number of cells&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So:&lt;/p&gt;

$$X^\top X = \begin{bmatrix} S_2 &amp; S \\ S &amp; n \end{bmatrix}$$


&lt;h5 id=&#34;34-compute&#34;&gt;3.4 Compute $X^\top u$&lt;/h5&gt;

$$X^\top u = \begin{bmatrix} \sum s_i u_i \\ \sum u_i \end{bmatrix}$$


&lt;p&gt;Let&amp;rsquo;s denote:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$U = \sum_{i=1}^n u_i$
&lt;/li&gt;
&lt;li&gt;$SU = \sum_{i=1}^n s_i u_i$
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So:&lt;/p&gt;

$$X^\top u = \begin{bmatrix} SU \\ U \end{bmatrix}$$


&lt;h5 id=&#34;35-solve&#34;&gt;3.5 Solve $\theta = (X^\top X)^{-1} X^\top u$&lt;/h5&gt;
&lt;p&gt;The inverse of a $2\times2$
 matrix:&lt;/p&gt;

$$\begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}^{-1} = \frac{1}{ad-bc}\begin{bmatrix} d &amp; -b \\ -c &amp; a \end{bmatrix}$$


&lt;p&gt;So:&lt;/p&gt;

$$(X^\top X)^{-1} = \frac{1}{S_2n - S^2}\begin{bmatrix} n &amp; -S \\ -S &amp; S_2 \end{bmatrix}$$


&lt;p&gt;Now compute:&lt;/p&gt;

$$\theta = \frac{1}{S_2n - S^2}\begin{bmatrix} n &amp; -S \\ -S &amp; S_2 \end{bmatrix}\begin{bmatrix} SU \\ U \end{bmatrix}$$


&lt;p&gt;First row:

$$n\cdot SU - S\cdot U$$

&lt;/p&gt;
&lt;p&gt;Second row:

$$-S\cdot SU + S_2\cdot U$$

&lt;/p&gt;
&lt;h5 id=&#34;36-final-expression&#34;&gt;3.6 Final Expression&lt;/h5&gt;
&lt;p&gt;So we get:&lt;/p&gt;

$$\gamma_0 = \frac{n\cdot \sum s_i u_i - \sum s_i \cdot \sum u_i}{n\cdot \sum s_i^2 - (\sum s_i)^2} = \frac{\text{Cov}(u,s)}{\text{Var}(s)}$$



$$o = \bar{u} - \gamma_0\bar{s}$$


&lt;p&gt;This completes the derivation of the least squares solution with intercept, showing how to arrive at the covariance and variance expressions.&lt;/p&gt;
&lt;h3 id=&#34;why-normal-equations-in-32-in-section-d-give-the-optimal-solution&#34;&gt;Why Normal Equations in 3.2 in Section D give the optimal solution&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s prove that the normal equations&lt;/p&gt;

$$X^\top X\theta = X^\top u$$


&lt;p&gt;give us the optimal solution in least squares regression.&lt;/p&gt;
&lt;h4 id=&#34;goal-of-least-squares&#34;&gt;Goal of Least Squares&lt;/h4&gt;
&lt;p&gt;We want to minimize the sum of squared errors (residuals):&lt;/p&gt;

$$L(\theta) = \|u-X\theta\|^2 = (u-X\theta)^\top(u-X\theta)$$


&lt;p&gt;This is a quadratic loss function. To minimize it, we take its gradient with respect to $\theta$
, set it to zero, and solve.&lt;/p&gt;
&lt;h4 id=&#34;step-by-step-derivation&#34;&gt;Step-by-step Derivation&lt;/h4&gt;
&lt;p&gt;Start with:&lt;/p&gt;

$$L(\theta) = (u-X\theta)^\top(u-X\theta)$$


&lt;p&gt;Expand the product:&lt;/p&gt;

$$L(\theta) = u^\top u - 2\theta^\top X^\top u + \theta^\top X^\top X\theta$$


&lt;p&gt;Now take the gradient with respect to $\theta$
:&lt;/p&gt;

$$\nabla_\theta L = -2X^\top u + 2X^\top X\theta \tag{gradient} $$


&lt;p&gt;Set the gradient to zero:&lt;/p&gt;

$$-2X^\top u + 2X^\top X\theta = 0$$


&lt;p&gt;Divide both sides by 2:&lt;/p&gt;

$$X^\top X\theta = X^\top u$$


&lt;p&gt;This is the normal equation. When $X^\top X$
 is invertible (which it is in our case since $X$
 has full column rank), we can solve for $\theta$
:&lt;/p&gt;

$$\theta = (X^\top X)^{-1}X^\top u$$


&lt;p&gt;This solution minimizes the squared error because:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The loss function is convex (it&amp;rsquo;s quadratic and $X^\top X$
 is positive definite)&lt;/li&gt;
&lt;li&gt;We found where its gradient is zero&lt;/li&gt;
&lt;li&gt;The second derivative (Hessian) $2X^\top X$
 is positive definite&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Therefore, this solution gives us the global minimum of the least squares problem.&lt;/p&gt;
&lt;h4 id=&#34;derive-the-gradient-equation-above-for-the-least-squares-loss-function&#34;&gt;Derive the Gradient Equation above for the Least Squares Loss Function&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s walk through how we get:&lt;/p&gt;

$$\nabla_\theta L = -2X^\top u + 2X^\top X\theta$$


&lt;h5 id=&#34;step-1-write-the-loss-function&#34;&gt;Step 1: Write the Loss Function&lt;/h5&gt;
&lt;p&gt;The least squares loss is:&lt;/p&gt;

$$L(\theta) = \|u-X\theta\|^2 = (u-X\theta)^\top(u-X\theta)$$


&lt;h5 id=&#34;step-2-expand-the-quadratic-form&#34;&gt;Step 2: Expand the Quadratic Form&lt;/h5&gt;
&lt;p&gt;Let&amp;rsquo;s expand this expression:&lt;/p&gt;

$$L(\theta) = u^\top u - 2\theta^\top X^\top u + \theta^\top X^\top X\theta$$


&lt;p&gt;Here&amp;rsquo;s how we get this expansion:&lt;/p&gt;

$$(u-X\theta)^\top(u-X\theta) = u^\top u - 2\theta^\top X^\top u + \theta^\top X^\top X\theta$$


&lt;p&gt;This follows from two key matrix properties:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$(AB)^T = B^T A^T$
&lt;/li&gt;
&lt;li&gt;$u^\top X\theta = \theta^\top X^\top u$
 (scalar equality)&lt;/li&gt;
&lt;/ol&gt;
&lt;h5 id=&#34;step-3-take-the-gradient-with-respect-to&#34;&gt;Step 3: Take the Gradient with Respect to $\theta$&lt;/h5&gt;
&lt;p&gt;We differentiate term by term:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;First term:

   $$\nabla_\theta(u^\top u) = 0$$
   

because it doesn&amp;rsquo;t depend on $\theta$
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Second term:

   $$\nabla_\theta(-2\theta^\top X^\top u) = -2X^\top u$$
   

because it&amp;rsquo;s linear in $\theta$
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Third term:

   $$\nabla_\theta(\theta^\top X^\top X\theta) = 2X^\top X\theta \tag{quadratic grad.}$$
   

This is a quadratic form, and its derivative is a standard result from matrix calculus.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Adding all parts together:&lt;/p&gt;

$$\nabla_\theta L = -2X^\top u + 2X^\top X\theta$$


&lt;p&gt;This gradient expression gives us the direction of steepest ascent of the loss function at any point $\theta$
. Setting it to zero leads to the normal equations, which give us the optimal solution.&lt;/p&gt;
&lt;h5 id=&#34;note-vector-dot-product-commutativity&#34;&gt;Note: Vector Dot Product Commutativity&lt;/h5&gt;
&lt;p&gt;Please note the identity&lt;/p&gt;

$$a^\top b = b^\top a$$


&lt;p&gt;holds true. This is a foundational property in linear algebra, and the key point is:&lt;/p&gt;
&lt;p&gt;Both $a^\top b$
 and $b^\top a$
 are scalars (i.e., single numbers), and they are equal because they compute the same dot product. Let&amp;rsquo;s break it down:&lt;/p&gt;
&lt;p&gt;Let $a = [a_1, a_2, \ldots, a_n]^\top$
, and $b = [b_1, b_2, \ldots, b_n]^\top$
.&lt;/p&gt;
&lt;p&gt;Then:&lt;/p&gt;

$$a^\top b = \sum_{i=1}^n a_i b_i$$



$$b^\top a = \sum_{i=1}^n b_i a_i$$


&lt;p&gt;But since real number multiplication is commutative (i.e., $a_i b_i = b_i a_i$
), the two sums are identical:&lt;/p&gt;

$$a^\top b = b^\top a$$


&lt;h5 id=&#34;note-on-shapes-and-intuition&#34;&gt;Note on shapes and intuition:&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;$a^\top b$
 is a $1\times1$
 scalar (row vector √ó column vector)&lt;/li&gt;
&lt;li&gt;$b^\top a$
 is also a $1\times1$
 scalar (same logic)&lt;/li&gt;
&lt;li&gt;This is just the dot product: it doesn&amp;rsquo;t matter which order you take the dot product in. It&amp;rsquo;s symmetric. This property is crucial in many derivations in linear algebra and optimization, including our least squares derivation where we used $u^\top X\theta = \theta^\top X^\top u$
.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since they&amp;rsquo;re both just numbers, and equal by commutativity, the identity holds.&lt;/p&gt;
&lt;h3 id=&#34;derive-quadratic-form-gradient-in-equation-quadratic-grad&#34;&gt;Derive Quadratic Form Gradient in Equation (quadratic grad.)&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s prove the key gradient identity with Matrix Calculus:&lt;/p&gt;

$$\nabla_\theta(\theta^\top A\theta) = (A + A^\top)\theta$$


&lt;p&gt;And its special case when $A$
 is symmetric:&lt;/p&gt;

$$\nabla_\theta(\theta^\top A\theta) = 2A\theta$$


&lt;h4 id=&#34;setup&#34;&gt;Setup&lt;/h4&gt;
&lt;p&gt;Given:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\theta \in \mathbb{R}^p$
 is a column vector&lt;/li&gt;
&lt;li&gt;$A \in \mathbb{R}^{p \times p}$
 is a constant matrix&lt;/li&gt;
&lt;li&gt;The scalar function is $f(\theta) = \theta^\top A\theta = \sum_{i=1}^p \sum_{j=1}^p \theta_i A_{ij} \theta_j$
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;overview-of-steps&#34;&gt;Overview of Steps&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: Write in Summation Form&lt;/p&gt;

$$f(\theta) = \sum_{i=1}^p \sum_{j=1}^p \theta_i A_{ij} \theta_j$$


&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: Compute Partial Derivatives&lt;/p&gt;
&lt;p&gt;The gradient&amp;rsquo;s k-th component is:&lt;/p&gt;

$$\frac{\partial f}{\partial \theta_k} = \frac{\partial}{\partial \theta_k} \sum_{i=1}^p \sum_{j=1}^p \theta_i A_{ij} \theta_j$$


&lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: Use Product Rule&lt;/p&gt;
&lt;p&gt;When differentiating:&lt;/p&gt;

$$\frac{\partial}{\partial \theta_k}(\theta_i A_{ij} \theta_j) = A_{ij}(\delta_{ik}\theta_j + \theta_i\delta_{jk})$$


&lt;p&gt;where $\delta_{ik}$
 is the Kronecker delta:&lt;/p&gt;

$$\delta_{ik} = \begin{cases} 1 &amp; \text{if } i=k \\ 0 &amp; \text{otherwise} \end{cases}$$


&lt;p&gt;&lt;strong&gt;Step 4&lt;/strong&gt;: Sum Terms&lt;/p&gt;

$$\frac{\partial f}{\partial \theta_k} = \sum_{j=1}^p A_{kj}\theta_j + \sum_{i=1}^p \theta_i A_{ik}$$


&lt;p&gt;&lt;strong&gt;Step 5&lt;/strong&gt;: Express in Vector Form&lt;/p&gt;
&lt;p&gt;This gives us:&lt;/p&gt;

$$\frac{\partial f}{\partial \theta_k} = (A\theta)_k + (A^\top\theta)_k$$


&lt;p&gt;Therefore:&lt;/p&gt;

$$\nabla_\theta f = A\theta + A^\top\theta = (A + A^\top)\theta$$


&lt;p&gt;&lt;strong&gt;Symmetric Matrix&lt;/strong&gt;: a special case&lt;/p&gt;
&lt;p&gt;When $A = A^\top$
, we get:&lt;/p&gt;

$$\nabla_\theta(\theta^\top A\theta) = 2A\theta$$


&lt;p&gt;This is the form we use in least squares optimization where $A = X^\top X$
 is symmetric.&lt;/p&gt;
&lt;h4 id=&#34;detail-proof-of-key-steps-above&#34;&gt;Detail Proof of Key Steps above&lt;/h4&gt;
&lt;p&gt;Let&amp;rsquo;s prove that:&lt;/p&gt;

$$\frac{\partial}{\partial \theta_k} \left(\sum_{i=1}^p \sum_{j=1}^p \theta_i A_{ij} \theta_j\right) = \sum_{j=1}^p A_{kj}\theta_j + \sum_{i=1}^p \theta_i A_{ik}$$


&lt;h5 id=&#34;step-by-step-derivation-1&#34;&gt;&lt;strong&gt;Step-by-step Derivation&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;Start with the function:&lt;/p&gt;

$$f(\theta) = \sum_{i=1}^p \sum_{j=1}^p \theta_i A_{ij} \theta_j$$


&lt;p&gt;Take partial derivative with respect to $\theta_k$
, using product rule for all $\theta_i$
 and $\theta_j$
 terms:&lt;/p&gt;

$$\frac{\partial f}{\partial \theta_k} = \sum_{i=1}^p \sum_{j=1}^p A_{ij} \cdot \frac{\partial}{\partial \theta_k}(\theta_i \theta_j)$$


&lt;p&gt;Now:&lt;/p&gt;

$$\frac{\partial}{\partial \theta_k}(\theta_i \theta_j) = \delta_{ik}\theta_j + \theta_i\delta_{jk}$$


&lt;p&gt;where $\delta_{ik}$
 is the Kronecker delta:&lt;/p&gt;

$$\delta_{ik} = \begin{cases} 1 &amp; \text{if } i=k \\ 0 &amp; \text{otherwise} \end{cases}$$


&lt;p&gt;Therefore:&lt;/p&gt;

$$\frac{\partial f}{\partial \theta_k} = \sum_{i=1}^p \sum_{j=1}^p A_{ij}(\delta_{ik}\theta_j + \theta_i\delta_{jk})$$


&lt;p&gt;Split into two sums:&lt;/p&gt;
&lt;p&gt;First term:

$$\sum_{i=1}^p \sum_{j=1}^p A_{ij}\delta_{ik}\theta_j = \sum_{j=1}^p A_{kj}\theta_j$$

&lt;/p&gt;
&lt;p&gt;(since $\delta_{ik}=1$
 only when $i=k$
)&lt;/p&gt;
&lt;p&gt;Second term:

$$\sum_{i=1}^p \sum_{j=1}^p A_{ij}\theta_i\delta_{jk} = \sum_{i=1}^p A_{ik}\theta_i$$

&lt;/p&gt;
&lt;p&gt;(since $\delta_{jk}=1$
 only when $j=k$
)&lt;/p&gt;
&lt;h5 id=&#34;interim-result&#34;&gt;&lt;strong&gt;Interim Result&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;Combining both terms:&lt;/p&gt;

$$\frac{\partial f}{\partial \theta_k} = \sum_{j=1}^p A_{kj}\theta_j + \sum_{i=1}^p A_{ik}\theta_i$$


&lt;p&gt;In vector form, this means:&lt;/p&gt;

$$\nabla_\theta(\theta^\top A\theta) = A\theta + A^\top\theta = (A + A^\top)\theta$$


&lt;h5 id=&#34;from-component-wise-to-vector-form-of-gradient&#34;&gt;&lt;strong&gt;From Component-wise to Vector Form of Gradient&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;For each coordinate $k \in \{1,\ldots,p\}$
, we showed:&lt;/p&gt;

$$\frac{\partial f}{\partial \theta_k} = \underbrace{\sum_{j=1}^p A_{kj}\theta_j}_{(A\theta)_k} + \underbrace{\sum_{i=1}^p A_{ik}\theta_i}_{(A^\top\theta)_k}$$


&lt;p&gt;So each element of the gradient vector is:&lt;/p&gt;

$$(\nabla_\theta f)_k = (A\theta)_k + (A^\top\theta)_k$$


&lt;h5 id=&#34;stacking-into-vector-form&#34;&gt;&lt;strong&gt;Stacking into Vector Form&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;The full gradient vector is:&lt;/p&gt;
&lt;p&gt;
$$\nabla_\theta f = \begin{bmatrix} 
(A\theta)_1 + (A^\top\theta)_1 \\
(A\theta)_2 + (A^\top\theta)_2 \\
\vdots \\
(A\theta)_p + (A^\top\theta)_p
\end{bmatrix} = A\theta + A^\top\theta$$


The notation $(A\theta)_k$
 which refers to the k-th component of the matrix-vector product $A\theta$
. This follows because matrix-vector multiplication simply stacks the components:&lt;/p&gt;

$$(A\theta)_k = \sum_j A_{kj}\theta_j$$


&lt;p&gt;
$$A\theta = \begin{bmatrix} 
\sum_{j=1}^p A_{1j}\theta_j \\
\sum_{j=1}^p A_{2j}\theta_j \\
\vdots \\
\sum_{j=1}^p A_{pj}\theta_j
\end{bmatrix}$$


Similarly, for the transpose:

$$(A^\top\theta)_k = \sum_i A_{ik}\theta_i$$

&lt;/p&gt;
&lt;h5 id=&#34;final-result&#34;&gt;&lt;strong&gt;Final Result&lt;/strong&gt;&lt;/h5&gt;
&lt;p&gt;Therefore:&lt;/p&gt;

$$\nabla_\theta(\theta^\top A\theta) = A\theta + A^\top\theta = (A + A^\top)\theta$$


&lt;h3 id=&#34;from-normal-equations-to-standard-regression-form-36-in-section-d&#34;&gt;From Normal Equations to Standard Regression Form (3.6 in Section D)&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s derive how the normal equations solution:&lt;/p&gt;

$$\theta = \frac{1}{nS_2-S_1^2}\begin{bmatrix} n &amp; -S_1 \\ -S_1 &amp; S_2 \end{bmatrix}\begin{bmatrix} SU \\ U \end{bmatrix}$$


&lt;p&gt;relates to the standard regression form:&lt;/p&gt;

$$\gamma_0 = \frac{\text{Cov}(u,s)}{\text{Var}(s)} = \frac{n\sum s_i u_i - \sum s_i \sum u_i}{n\sum s_i^2 - (\sum s_i)^2}$$


&lt;p&gt;with offset:&lt;/p&gt;

$$o = \bar{u} - \gamma_0\bar{s}$$


&lt;h4 id=&#34;given-matrices-and-vectors&#34;&gt;Given Matrices and Vectors&lt;/h4&gt;
&lt;p&gt;Let $s=[s_1,\ldots,s_n]^T$
, $u=[u_1,\ldots,u_n]^T$
&lt;/p&gt;
&lt;p&gt;Model:

$$u_i = \gamma_0 s_i + o + \varepsilon_i \text{ or } u = \gamma_0 s + o\cdot 1 + \varepsilon$$

&lt;/p&gt;
&lt;h4 id=&#34;matrix-form&#34;&gt;Matrix Form&lt;/h4&gt;
&lt;p&gt;Design matrix:

$$X = [s \quad 1] \in \mathbb{R}^{n\times 2}$$

&lt;/p&gt;
&lt;p&gt;Parameters:

$$\theta = \begin{bmatrix} \gamma_0 \\ o \end{bmatrix}$$

&lt;/p&gt;
&lt;h4 id=&#34;computing-products&#34;&gt;Computing Products&lt;/h4&gt;

$$X^T X = \begin{bmatrix} \sum s_i^2 &amp; \sum s_i \\ \sum s_i &amp; n \end{bmatrix} = \begin{bmatrix} S_2 &amp; S_1 \\ S_1 &amp; n \end{bmatrix}$$



$$X^T u = \begin{bmatrix} \sum s_i u_i \\ \sum u_i \end{bmatrix} = \begin{bmatrix} SU \\ U \end{bmatrix}$$


&lt;h4 id=&#34;solution-components&#34;&gt;Solution Components&lt;/h4&gt;
&lt;p&gt;First row (slope):

$$\gamma_0 = \frac{1}{nS_2-S_1^2}(n\cdot SU - S_1\cdot U)$$

&lt;/p&gt;
&lt;p&gt;Second row (intercept):

$$o = \frac{1}{nS_2-S_1^2}(-S_1\cdot SU + S_2\cdot U)$$

&lt;/p&gt;
&lt;h4 id=&#34;statistical-interpretation&#34;&gt;Statistical Interpretation&lt;/h4&gt;
&lt;p&gt;Define:

$$\bar{s} = \frac{1}{n}\sum s_i \quad \bar{u} = \frac{1}{n}\sum u_i$$

&lt;/p&gt;

$$\text{Var}(s) = \frac{1}{n}\sum(s_i-\bar{s})^2 = \frac{1}{n}S_2 - \bar{s}^2$$



$$\text{Cov}(u,s) = \frac{1}{n}\sum(u_i-\bar{u})(s_i-\bar{s}) = \frac{1}{n}SU - \bar{u}\bar{s}$$


&lt;h4 id=&#34;final-result-1&#34;&gt;Final Result&lt;/h4&gt;
&lt;p&gt;This gives us:

$$\gamma_0 = \frac{\text{Cov}(u,s)}{\text{Var}(s)} \text{ and } o = \bar{u} - \gamma_0\bar{s}$$

&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
